{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1: Textsegmentering och Korpusanalys\n",
    "\n",
    "Från ett datorperspektiv är en text i första hand en sekvens av tecken, såsom bokstäver och siffror. Innan vi kan bearbeta en text med språkteknologiska verktyg behöver vi dela upp den i lingvistiskt mer meningsfulla enheter, såsom stycken, meningar eller ord. När enheterna är just ord kallas denna segmentering för **tokenisering**. I denna laboration ska ni implementera en enkel tokeniserare för löpande text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Läs in råtexten\n",
    "\n",
    "Texten som ni ska jobba med i den första uppgiften är en artikel från svenska Wikipedia: [Gustav III](https://sv.wikipedia.org/wiki/Gustav_III). På en Wikipedia-sida förekommer inte bara text utan även andra data, såsom bilder och tabeller. Innan ni kan tokenisera texten skulle ni därför egentligen först behöva extrahera den från sidan. För just den här uppgiften har vi dock redan gjort detta för er. Den extraherade texten finns i filen `text1.txt`.\n",
    "\n",
    "För att läsa in den extraherade texten i Python definierar vi en hjälpfunktion `read_data()`. Funktionen öppnar den angivna filen och returnerar dess innehåll som en lista med textrader, en sträng per rad. I textfilen avslutas varje rad med nyrad-tecknet (`\\n`); detta tecken tas bort med hjälp av [`rstrip()`](https://docs.python.org/3/library/stdtypes.html#str.rstrip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename) as f:\n",
    "        return [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kan ni läsa in råtexten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = read_data('text1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nästa cellen skriver ut de första 50 raderna i texten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text1[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Läs in guldstandarden\n",
    "\n",
    "Till råtexten finns även en guldstandardtokenisering. Denna tokenisering följer de regler som används i [Stockholm–Umeå Corpus (SUC)](https://spraakbanken.gu.se/swe/resurs/suc3), en standardkorpus för svenska. Filen med guldstandardtokeniseringen innehåller alla token i råtexten, ett token per rad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold1 = read_data('gold1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titta på denna guldstandard och försök att förstå de principer den bygger på. De flesta token är vanliga ord eller skiljetecken, men notera att förkortningar behandlas som ett token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gold1[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mellanrumsbaserad tokenisering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nästa cell innehåller en mycket enkel tokeniserare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ws(lines):\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        for token in line.split():\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denna funktion tar in en lista med textrader, bryter upp varje rad vid mellanrum (*whitespace*) genom att anropa metoden [`split()`](https://docs.python.org/3.5/library/stdtypes.html#str.split) och samlar in de resulterande strängarna i en lista `tokens`.\n",
    "\n",
    "### Jämför tokeniseringen med guldstandarden\n",
    "\n",
    "Provkör tokeniseraren på de 50 första raderna i texten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize_ws(text1[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jämför denna tokenisering med guldstandarden. Vilka skillnader finns?\n",
    "\n",
    "De flesta skillnaderna kommer att vara fall av **undersegmentering**, där tokeniseraren har missat att dela upp ett token. Motsatsen till detta är **översegmentering**, där tokeniseraren har delat upp det som egentligen borde ha varit ett token i flera.\n",
    "\n",
    "För att undersöka skillnaderna kan ni använda funktionen `diff()` från labmodulen. Denna funktion tar två argument, en lista med guldstandardtoken och en lista med automatiskt predicerade token, och returnerar en ny lista som beskriver skillnaderna mellan dessa två tokeniseringarna på ett kompakt sätt. Följande kommando t.ex. ger er de första tio skillnaderna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab1\n",
    "\n",
    "lab1.diff(gold1, tokenize_ws(text1))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listan innehåller  par vars första komponent är en sekvens av token som förekommer i guldstandarden men inte förekommer i den automatiska tokeniseringen, och vars andra komponent är en sekvens som förekommer i den automatiska tokeniseringen men inte i guldstandarden. Följande kodbit skriver ut listan så att den blir mera läsbar; den skriver även ut antalet token i respektive sekvens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatera en lista med token.\n",
    "def fmt_tokens(tokens):\n",
    "    return ' '.join(tokens) + ' ({})'.format(len(tokens))\n",
    "\n",
    "# Skriv ut information om avvikande delsekvenser.\n",
    "print('Gold tokens'.ljust(40), 'Predicted tokens'.ljust(40))\n",
    "print()\n",
    "for gold_tokens, pred_tokens in lab1.diff(gold1, tokenize_ws(text1)):\n",
    "    print(fmt_tokens(gold_tokens).ljust(40), fmt_tokens(pred_tokens).ljust(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersök skillnaderna mellan guldstandarden och den mellanrumsbaserade tokeniseringen. Leta efter exempel för över- och undersegmentering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beräkna precision och täckning\n",
    "\n",
    "I Uppgift&nbsp;1 gjorde ni vad man kallar en *kvalitativ* utvärderingen av er tokeniserare. Ett sätt att göra en mera kvantitativ utvärdering är att räkna ut dess **precision** och dess **täckning** (recall). Precision är definierad som andelen korrekt identifierade token bland alla token som tokeniseraren producerar. Ett token räknas som korrekt identifierat när det täcker samma teckenpositioner i texten som ett token i guldstandarden. Recall är definierad som andelen korrekt identifierade token bland alla token som finns i guldstandarden. För att beräkna dessa värden kan ni använda följande kodcell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ws = tokenize_ws(text1)\n",
    "\n",
    "print('Errors: {}'.format(lab1.n_errors(gold1, tokens_ws)))\n",
    "print('Precision: {:.4f}'.format(lab1.precision(gold1, tokens_ws)))\n",
    "print('Recall: {:.4f}'.format(lab1.recall(gold1, tokens_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisering baserad på reguljära uttryck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I andra delen av denna laboration ska ni ersätta den enkla mellanrumsbaserade tokeniseringen med en mera avancerad tokenisering baserad på **reguljära uttryck**. Om ni känner att ni behöver öva mer på reguljära uttryck innan ni kan göra uppgiften kom ihåg de rekommenderade webbsidor: [Regex Golf](https://alf.nu/RegexGolf) och [Regex 101](https://regex101.com).\n",
    "\n",
    "Innan ni kan använda reguljära uttryck i Python måste ni först ladda den relevanta modulen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En enkel tokeniserare baserad på reguljära uttryck ser ut så här:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_re(regex, lines):\n",
    "    output = []\n",
    "    for line in lines:\n",
    "        for match in re.finditer(regex, line):\n",
    "            output.append(match.group(0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denna funktion hittar alla längsta, icke-överlappande förekomster av mönstret `regex` i raden `line` och returnerar dem som en lista. Raden skannas från vänster till höger och de matchande delsträngarna returneras i samma ordning.\n",
    "\n",
    "För att återskapa och köra den mellanrumsbaserade tokeniseraren med hjälp av reguljära uttryck kan ni använda följande kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reguljärt uttryck som tokeniseraren ska använda. TODO: Modifiera den för att få bättre resultat. \n",
    "regex = r'\\S+'\n",
    "\n",
    "tokens_re = tokenize_re(regex, text1)\n",
    "\n",
    "print('Errors: {}'.format(lab1.n_errors(gold1, tokens_re)))\n",
    "print('Precision: {:.4f}'.format(lab1.precision(gold1, tokens_re)))\n",
    "print('Recall: {:.4f}'.format(lab1.recall(gold1, tokens_re)))\n",
    "\n",
    "# För att felsöka regexen kan det vara bra att kommentera in följande rad:\n",
    "# lab1.diff(gold1, tokens_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uppgift 1**: Hitta ett reguljärt uttryck som eliminerar så många skillnader mellan guldstandarden och den automatiska tokeniseringen som möjligt. Använd era insikter från Uppgift&nbsp;1. Er färdiga tokeniserare bör komma upp i minst 99,5% precision och täckning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utvärdera den förbättrade tokeniseraren på en ny text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uppgift 2**: **Utvärdera er tokeniserare på en andra artikel från svenska Wikipedia:\n",
    "[Katarina II av Ryssland](https://sv.wikipedia.org/wiki/Katarina_II_av_Ryssland). (Hon var förresten kusin till Gustav&nbsp;III.) Ange även här antal fel, precision och täckning. \n",
    "\n",
    "Råtext och guldstandardtokeniseringen laddar ni så här:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = read_data('text2.txt')\n",
    "gold2 = read_data('gold2.txt')\n",
    "\n",
    "# TODO: Skriv kod för att utvärdera tokeniseraren på den nya texten här."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Korpusanalys\n",
    "\n",
    "Nu har ni en tokeniserare och är redo för att skapa en egen liten korpus och göra en statistisk analys på den! \n",
    "\n",
    "### Bygga Korpusen\n",
    "\n",
    "**Uppgift 3**: Ladda ner minst 10 (längre) Wikipediaartiklar och extrahera råtexten. Använd kodskeletten i cellerna nedan och ersätt Kapybara-artikeln med era valda artiklar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ladda ner 10 artiklar genom att lägga till dem i listan `urls`.\n",
    "\n",
    "import requests\n",
    "\n",
    "urls = [\"https://sv.wikipedia.org/wiki/Kapybara\"]\n",
    "\n",
    "for url in urls: \n",
    "    response = requests.get(url)\n",
    "\n",
    "    with open(\"corpus.html\", 'a', encoding='utf-8') as file:\n",
    "        file.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu ska vi bli av med html-annoteringarna och bara spara texten i paragraferna: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extrahera råtexten från alla 10 artiklar.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"corpus.html\", 'r', encoding='utf-8') as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "cleaned_text = '\\n'.join([para.get_text() for para in paragraphs])\n",
    "\n",
    "with open(\"corpus.txt\", 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Nu har ni skapat en egen liten korpus och sparat resultatet i `corpus.txt`. Nu behöver vi tokenisera texten för att kunna göra en ordfrekvensanalys: \n",
    "\n",
    "**Uppgift 4**: Använd eran tokeniserare på korpusen. För att göra detta, anropa funktionen `tokenize_re` från tidigare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenisera texterna. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ordfrekvensanalys\n",
    "\n",
    "Nu ska ni räkna alla ord i dem 10 artiklarna. Använd gärna en [Counter](https://docs.python.org/3/library/collections.html#counter-objects) från Python's *collections* library på listan för att enkelt räkna dem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uppgift 5**: Använd tokenlistan från uppgift 4 och spara ordfrekvenserna över hela korpusen i en Counter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# TODO: Skapa en Counter som innehåller ordfrekvenserna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uppgift 6** Titta på resultatet. Ni kan kolla på dem mest frekventa orden med Counter's [`most_common()`](https://docs.python.org/3/library/collections.html#collections.Counter.most_common) funktion. Tycker ni att korpusen uppfyller [Zipf’s law](https://en.wikipedia.org/wiki/Zipf%27s_law) och/eller [Zipf’s brevity law](https://en.wikipedia.org/wiki/Brevity_law) som vi pratade om i föreläsningen? Varför (inte)? \n",
    "\n",
    "TODO: Svara på frågan och reflektera resultatet i 2-3 meningar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titta närmare på Countern i den här cellen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skriv analysen i den här cellen. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
